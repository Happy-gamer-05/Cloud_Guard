input {
  s3 {
    # The name of your S3 bucket
    bucket => "project-s3-bucket-logs-all"
    access_key_id => ${AWS_ACCESS_KEY_ID}
    secret_access_key => ${AWS_SECRET_ACCESS_KEY}
    # The AWS region of your S3 bucket.
    region => "us-east-1"
    prefix => "2025/"
    gzip_pattern => ".*"
    codec => "json"
    tags => ["s3_direct", "cloud"]
  }
}

filter {
  split {
    field => "logEvents"
  }

  mutate {
    add_field => { "message" => "%{[logEvents][message]}" }
    remove_field => ["logEvents", "logGroup", "logStream"] # Remove unnecessary fields
  }

  
  grok {
    match => {
      "message" => [
        "%{IPORHOST:client_ip} - - \[%{HTTPDATE:timestamp}\] \"%{WORD:method} %{URIPATHPARAM:request} HTTP/%{NUMBER:http_version}\" %{NUMBER:status} %{NUMBER:bytes} \"%{DATA:referrer}\" \"%{DATA:useragent_string}\"",
        "%{IPORHOST:client_ip} - - \[%{HTTPDATE:timestamp}\] \"%{DATA:request}\" %{NUMBER:status} %{NUMBER:bytes} \"%{DATA:referrer}\" \"%{DATA:useragent_string}\""
      ]
    }
    tag_on_failure => ["_grokparsefailure"]
  }

  mutate {
    add_field => {
      "[source][address]" => "%{client_ip}"
      "[http][request][method]" => "%{method}"
      "[http][version]" => "%{http_version}"
      "[url][original]" => "%{request}"
      "[http][request][referrer]" => "%{referrer}"
      "[user_agent][original]" => "%{useragent_string}"
      "[http][response][status_code]" => "%{status}"
      "[http][response][body][bytes]" => "%{bytes}"
    }
  }

  # geo ip filter. This will enrich the data with location info.
  geoip {
    source => "client_ip"
    target => "source.geo"
    database => "/usr/share/logstash/GeoLite2-City.mmdb"
  }

  # useragent filter. This will parse the useragent string.
  useragent {
    source => "useragent_string"
    target => "user_agent"
  }
}


output {
  elasticsearch {
    hosts => ["http://elasticsearch:9200"]
    index => "cloud-logs-%{+YYYY.MM.dd}"
  }

  file {
    path => "/tmp/s3-debug.log"
  }

  stdout {
    codec => rubydebug
  }
}
